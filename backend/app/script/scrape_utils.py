import os
from langchain_community.document_loaders import PlaywrightURLLoader

FILE = "links_file.txt"

# Scraper chaque URL s√©par√©ment pour ne pas perdre les donn√©es en cas d‚Äô√©chec
current_dir = os.path.dirname(__file__)  # R√©cup√®re le dossier du script
output_folder = os.path.join(current_dir, "..","..", "api")  # Aller vers "output"
output_file = os.path.join(output_folder, "result.txt")

file_path = os.path.join(current_dir, FILE)
with open(file_path, 'r') as file:  
    urls = [line.strip() for line in file] 

# Nombre max de tentatives par page
MAX_RETRIES = 3  

# Stocker les r√©sultats
successful_documents = []
failed_urls = []

def load_page_with_retry(url, retries=MAX_RETRIES):
    """Charge une page et relance Playwright en cas de crash."""
    attempt = 0
    while attempt < retries:
        try:
            print(f"üîÑ Chargement de {url} (Tentative {attempt+1}/{retries})...")

            # Cr√©er un nouveau PlaywrightURLLoader √† chaque tentative
            loader = PlaywrightURLLoader(urls=[url], remove_selectors=["header","footer","div.lc-inline_column_first-edit","div.lc-inline_section-edit","div.inline_container",\
                                                                       "a.FootnoteRef","div.lc-inline_column_first-content-edit","a.next-epc","a.back-epc"])
            data = loader.load()
            
            print(f"‚úÖ Succ√®s : {url}")
            return data  # Retourne le document r√©cup√©r√©

        except Exception as e:
            print(f"‚ùå Erreur sur {url}: {e}")
            attempt += 1            
            if attempt >= retries:
                print(f"üö® √âchec final : {url}")
                return None  # √âchec d√©finitif apr√®s `MAX_RETRIES`

if os.path.exists(output_file):
    os.remove(output_file)

for url in urls:
    result = load_page_with_retry(url)
    if result:
        successful_documents.extend(result)  
        with open(output_file, 'a') as file:
            file.write(result[0].page_content)
    else:
        failed_urls.append(url)  # Stocker les URLs √©chou√©es
        print("Erreur : Aucun contenu r√©cup√©r√©.")
        
        


print(f"\n‚úÖ {len(successful_documents)} pages charg√©es avec succ√®s.")
print(f"‚ùå {len(failed_urls)} pages ont √©chou√© : {failed_urls}")


# √âcrire dans le fichier situ√© dans "output/"

print(f"Les URLs ont √©t√© enregistr√©es dans {output_file}")